{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc3a62b",
   "metadata": {},
   "source": [
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import hail as hl\n",
    "from bokeh.io import output_notebook,show\n",
    "import gnomad.utils.vep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec77f44",
   "metadata": {},
   "source": [
    "## 2. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c551327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gnomaAD v.3.1.2\n",
    "ht = hl.read_table('gs://gcp-public-data--gnomad/release/3.1.2/ht/genomes/gnomad.genomes.v3.1.2.sites.ht')\n",
    "ht = ht.head(100000) # Subset the data\n",
    "\n",
    "# Import mutation rates from gnomAD paper\n",
    "ht_mu = hl.import_table('data/supplementary_dataset_10_mutation_rates.tsv.gz',\n",
    "                delimiter='\\t', impute=True, force_bgz=True)\n",
    "\n",
    "# Import context table from gnomad (https://broadinstitute.github.io/gnomad_methods/api_reference/utils/vep.html?highlight=context#gnomad.utils.vep.get_vep_context)\n",
    "context_table = gnomad.utils.vep.get_vep_context(\"GRCh38\").ht()\n",
    "context_table_parsed = context_table.select(context_table.context)\n",
    "context_table_parsed = context_table_parsed.transmute(context = context_table_parsed.context[2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac9427",
   "metadata": {},
   "source": [
    "### Show the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47edc9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table with methylation level and mutational rate in the trinucleotide context\n",
    "ht_mu.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This table contains already precalculated nucleotides -3/+3 from mutation site \n",
    "context_table_parsed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b098b5d6",
   "metadata": {},
   "source": [
    "## 3. Add context field to main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c322ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before joining the tri-nucleotide context of mutation\n",
    "ht.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join only matching rows from context to ht table.\n",
    "ht = ht.key_by('locus', 'alleles').join(context_table_parsed.key_by('locus', 'alleles'), how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad60f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After\n",
    "ht.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb6cadf",
   "metadata": {},
   "source": [
    "## 4. Add mutation rates for added contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b14ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split alleles field to ref and alt allele\n",
    "ht = ht.annotate(ref=ht.alleles[0], alt=ht.alleles[1])\n",
    "\n",
    "# Add mutation rates according to the context, but also ref and alt allele for this context\n",
    "ht = ht.key_by(\"context\", \"ref\", \"alt\").join(ht_mu.key_by(\"context\", \"ref\", \"alt\"), how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53998a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After adding context and mutation rates to the main table \n",
    "# (can be more than original number of rows as context may occure more than once depending on the locus)\n",
    "ht.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add merged context with ref and alt variants for later table with mutation rates\n",
    "ht = ht.annotate(context_ref_alt = ht.context + '_' + ht.ref + '_' + ht.alt)\n",
    "\n",
    "# Change key for grouping to merged context\n",
    "ht = ht.key_by('context_ref_alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e88058",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show that contexts may be the same, but locus is completely different\n",
    "ht.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5951266",
   "metadata": {},
   "source": [
    "#### *From this point the main key to group tables is by `context_ref_alt`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f2a5cd",
   "metadata": {},
   "source": [
    "## 5. Train linear model on synonymous variants for mutational class correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only synonymous variants\n",
    "ht_syn = ht.filter(ht.vep.most_severe_consequence == \"synonymous_variant\")\n",
    "\n",
    "# Calculate number of variants in each tri-nucleotide context in synonymous variants\n",
    "ht_syn_N_variants = (ht_syn.group_by(ht_syn.context_ref_alt).aggregate(N_variants = hl.agg.count()))\n",
    "\n",
    "# Calculate number of singletons for each tri-nucleotide context in synonymous variants\n",
    "ht_syn_singletons = ht_syn.filter(ht_syn.info.singleton == 1)\n",
    "ht_syn_N_singletons = (ht_syn_singletons.group_by(ht_syn_singletons.context_ref_alt).aggregate(N_singletons = hl.agg.count()))\n",
    "\n",
    "# Merge the N variants and N singletons tables\n",
    "ht_syn_ps  = ht_syn_N_variants.join(ht_syn_N_singletons, how = 'outer') # outer as all will match and we want all\n",
    "ht_syn_ps = ht_syn_ps.annotate(ps = ht_syn_ps.N_singletons/ht_syn_ps.N_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b309f0b4",
   "metadata": {},
   "source": [
    "### Show input table for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows before adding mutation rates\n",
    "ht_syn_ps.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add mutation rate back to the table\n",
    "#ht_syn_ps = ht_syn_ps.join(ht.select(ht.mu_snp), how = 'left')\n",
    "# With the code below it doesn't keep duplicates, which is good as main table has contexts annotated to more\n",
    "# than one variant, causing the duplication as the mu_snp is sometimes more than once appearing in the table)\n",
    "ht_syn_ps = ht_syn_ps.annotate(**ht.select(ht.mu_snp)[ht_syn_ps.context_ref_alt])\n",
    "\n",
    "#ht1.annotate(**ht2[ht1.x1])\n",
    "\n",
    "ht_syn_ps.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77039752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows after adding mutation rates\n",
    "ht_syn_ps.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6657abe3",
   "metadata": {},
   "source": [
    "### Perform regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954471ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform regression\n",
    "ht_syn_lm = ht_syn_ps.aggregate(hl.agg.linreg(ht_syn_ps.ps, [1, ht_syn_ps.mu_snp], weight=ht_syn_ps.N_variants).beta)\n",
    "\n",
    "# Show intercept and beta\n",
    "ht_syn_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70283c94",
   "metadata": {},
   "source": [
    "## 6. Predict expected number of variants for each context\n",
    "\n",
    "### For testing purposes focus on `upstream_gene_variant`\n",
    "\n",
    "### Function for regression eventually will be made starting here and put in `/utils/utils.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e03b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter specific variant functionall class for calculating MAPS in\n",
    "ht_reg_table = ht.filter(ht.vep.most_severe_consequence == \"upstream_gene_variant\")\n",
    "\n",
    "# Count number of variants and singletons\n",
    "ht_reg_table_N_variants = (specific.group_by(specific.context_ref_alt).aggregate(N_variants = hl.agg.count()))\n",
    "ht_reg_table_N_singletons = (specific.group_by(specific.context_ref_alt).aggregate(N_singletons = hl.agg.sum(specific.info.singleton)))\n",
    "\n",
    "# Merge the tables to obtain proportions (ps)\n",
    "ht_reg_table_ps = ht_reg_table_N_variants.join(ht_reg_table_N_singletons, how = \"outer\") # outer as we want all anyway\n",
    "ht_reg_table_ps = ht_reg_table_ps.annotate(ps = ht_reg_table_ps.N_singletons/ht_reg_table_ps.N_variants)\n",
    "\n",
    "# Add mutation rate back to the table (mu_snp matching key from variants table)\n",
    "#ht_reg_table_ps = ht_reg_table_ps.join(ht.select(ht.mu_snp), how = 'left')\n",
    "# With the code below it doesn't keep duplicates, which is good as main table has contexts annotated to more\n",
    "# than one variant, causing the duplication as the mu_snp is sometimes more than once appearing in the table)\n",
    "ht_reg_table_ps = ht_reg_table_ps.annotate(**ht.select(ht.mu_snp)[ht_reg_table_ps.context_ref_alt])\n",
    "\n",
    "# Get expected number of singletons by applying the model factors\n",
    "ht_reg_table_ps_lm = ht_reg_table_ps.annotate(expected_singletons=(ht_reg_table_ps.mu_snp * ht_syn_lm[1] + ht_syn_lm[0]) * ht_reg_table_ps.N_variants)\n",
    "\n",
    "# Now aggregate into the main consequence from contexts\n",
    "ht_reg_table_ps_lm_cons = ht_reg_table_ps_lm.annotate(consequence = \"upstream_gene_variant\")\n",
    "\n",
    "# To aggregate just sum for the context\n",
    "ht_reg_table_ps_lm_cons_agg = (ht_reg_table_ps_lm_cons.group_by(\"consequence\")\n",
    "              .aggregate(N_singletons=hl.agg.sum(ht_reg_table_ps_lm_cons.N_singletons),\n",
    "                         expected_singletons=hl.agg.sum(ht_reg_table_ps_lm_cons.expected_singletons),\n",
    "                         N_variants=hl.agg.sum(ht_reg_table_ps_lm_cons.N_variants)))\n",
    "\n",
    "# Calculate MAPS and aggregated proportions \n",
    "ht_reg_table_ps_lm_cons_agg_MAPS = ht_reg_table_ps_lm_cons_agg.annotate(ps_agg=ht_reg_table_ps_lm_cons_agg.N_singletons / ht_reg_table_ps_lm_cons_agg.N_variants,\n",
    "    maps=(ht_reg_table_ps_lm_cons_agg.N_singletons - ht_reg_table_ps_lm_cons_agg.expected_singletons) / ht_reg_table_ps_lm_cons_agg.N_variants)\n",
    "\n",
    "# Add MAPS standard error of the mean (sem)\n",
    "ht_reg_table_ps_lm_cons_agg_MAPS = ht_reg_table_ps_lm_cons_agg.annotate(maps_sem=(ht_reg_table_ps_lm_cons_agg.ps_agg * (1 - ht_reg_table_ps_lm_cons_agg.ps_agg) / ht_reg_table_ps_lm_cons_agg.N_variants) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final result\n",
    "ht_reg_table_ps_lm_cons_agg_MAPS.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function definition for the future\n",
    "#def Regress(name):\n",
    "#    \"\"\"\n",
    "#    Regress ...\n",
    "#    \"\"\"\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
